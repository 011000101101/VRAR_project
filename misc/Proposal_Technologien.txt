Überlegungen zur geplanten Implementierung:

Das System wird im Zuge dieser Arbeit nur recht rudimentär implementiert, um den Arbeitsaufwnd im Rahmen zu halten. 
So ist es nicht gegenstand dieser Arbeit, einen nahezu perfekten Machine Translator zu entwickeln, um im Kontext die richtigen Lesungen zu produzieren, da gelegentliche Fehlinterpretationen keinen Schaden anrichten und normalerweise vom Nutzer erkannt und durch Interaktion korrigiert werden können. 
Ebenso kann die Schriftzeichenerkennung mit steigenden Ansprüchen beliebig komplex werden. Da der Fokus jedoch auf dem Erkennen von Maschinenschrift liegt, ist beispielsweise die Erkennung handschriftlicher Zeichen vorerst nicht nötig, würde jedoch eine nette Erweiterungsmöglichkeit bieten, sollte das System den Erwartungen gerecht werden und Weiterentwicklung auch nach Ende dieser Arbeit erfolgen.
Um die Komponenten des Systems vorerst möglichst simpel zu halten und dennoch spätere Verbesserungen und Erweiterungen nicht auszuschließen werden wir von Beginn an auf gute Dokumentation und klare Schnittstellen achten. Außerdem werden Schnittstellen so allgemein wie möglich definiert, ohne die Performanz des Systems stark zu beeinträchtigen: Da die Bottlenecks bei den Bilderkennungs und -klassifizierungs-Subsystemen erwartet werden, nicht jedoch bei dem vergleichsweise algorithmisch weniger aufwendigen Machine-Translation-Subsystem, soll die Schnittstelle hier auch ausführliche (globale) Kontextinformationen zu jeder Anfrage bereitstellen, wobei vorerst nur ein sehr lokaler Kontext verwendet werden wird (Details in der Beschreibung des entsprechenden Subsystems).

Bisher sind noch keine Endgültigen Entscheidungen zum Design der Subsysteme getroffen worden, es wurden jedoch bereits teils mehr, teils weniger tiefe Überlegungen angestellt, die im folgenden beschrieben werden:

1. Segmentierungs-Subsystem:
Um das System zu vereinfachen könnte hier die Beobachtung verwendet werden, dass die Eingabebilder meistens nur Text in einer einzigen (oder zumindest sehr wenig verschiedenen) Schriftgrößen enthalten. Daher kann die Schriftgröße eventuell entweder automatisch erkannt oder vom Nutzer eingestellt (einfacher Slider zum Skalieren) werden. Dies würde es erlauben, Das Bild mit Filtern fixer größe zu scannen, was sowohl den implementierungs- (evtl. lern-) Aufwand, als auch die Laufzeit des Subsystems verringert.
Zur Implementierung dieses Systems wurden noch keine weitergehenden Überlegungen angestellt, die Intuition schlägt jedoch ein Convolutional Neuronal Network vor, Trainingsdaten hierfür könnten recht einfach generiert werden, indem Schriftzeichen gerendert und in "Hintergrundbildern" verteilt werden. 

2. Klassifizierungs-Subsystem:
Wir schränken unser System vorerst darauf ein, nur schwarze Schriftzeichen auf weißem (oder "hellem", helles grau bei schwarz/weiß, helle Farben bei vollfarb-Bildern) Hintergrund zumindest innerhalb der Bounding Box eines jeden Schriftzeichens, zu erkennen. Bei einem Buch z.B. ist dies Ohnehin gegeben, und auch bei Manga befindet sich der Kanjihaltige Text in "Sprechblasen" oder dedizierten Textfeldern (zwar enthalten Manga auch des öfteren Text direkt auf den Zeichnungen, hierbei handelt es sich jedoch generell um "geschriebene Soundeffekte" (ähnlich wie in Comics), und diese Art von Text enthält normalerweise keine Kanji. Wir denken, dass diese Einschränkung auch beim verarbeiten von Werbeanzeigen, Flyern, etc. die Nutzbarkeit nicht einschränkt, da diese Einschränkung im allgemeinen eine Bedingung für gute lesbarkeit des Originalmaterials darstellt.
Durch diese Einschränkung lässt sich jedoch die Klassifizierung deutlich vereinfachen. Unsere ursprüngliche Überlegung bestand darin, die erkannten Segmente via thresholding in Binärwertige Bilder umzuwandeln und auf Gesamtgröße und Scale zu normalisieren, und die Klassifizierung anschließend mit einem Naive Bayes Klassifier über die binären Pixelwerte durchzuführen. Dieser Klassifizierer wäre mit aus verschiedenen Fonts generierten Samplen für jedes distinkte Schriftzeichen (jede Klasse) trainiert worden, und das augmentieren der trainingsdaten durch hinzufügen einer geringen menge Noise hätte für ein gewisses Maß an robustheit bei der Übertragung auf echte Scans/Fotos gesorgt.
Durch initiale Tests haben wir jedoch festgestellt, dass kleine Schriftgrößen und nicht perfekte Aufnahmen oder Komprimierung der Quellbilder ein klares Trennen der Schriftzeichen vom Hintergrund mittels simplem thresholding verhindert.
[Bilder]
Dennoch bietet das Wissen, dass Schriftzeichen immer dunkel und Hintergrund immer hell sind, Potential zur vereinfachung des Systems, das wir nutzen wollen.
Eine neue Überlegung besteht darin, nicht die realen an die Trainingsdaten, sondern die Trainings- an die realdaten anzupasen, indem zusätzlich zu noise auch (potentiell ungleichmäßiger) blur hinzugefügt wird. Hier werden wir zunächst prüfen, ob sich ein Naive Bayes Classifier auch für (kleine) Greyscale-Bilder eignet, und andernfalls nach geeigneteren Klassifikatoren suchen.
Die Normalisierung der Segmente auf Größe, Auflösung und Scale (Proportion Schriftzeichengröße zu Bounding Box, z.B. konstanter Abstand zu den rändern) bleibt fester Teil unserer Überlegungen.

3. Machine-Translation-Subsystem: (mangels eines besseren Namens, denn hier findet ja keine übersetzung im klassischen Sinnse statt, sondern eher eine kontextsencsitive Transskription)
Die Lesung eines Kanji kann für gewöhnlich bereits nur mithilfe des lokalen Kontexts bestimmt werden. 
Während es gelegentlich vorkommen kann, dass die tatsächliche Lesung von der durch einfache Regeln abgeleiteten abweicht, so wird dies im allgemeinen entweder vom Nutzer festgestellt und die Lesung kann manuell inferiert werden, oder der Unterschied hindert nicht das verständnis des Textes (z.B. bei Namen, bei denen sehr eigenartige Lesungen auftreten können). Eine Klasse von Texten, bei der solche "schweren Lesungen" häufig vorkommen, sind poetische Texte, da eine abweichende Lesung hier oftmals als Stilmittel oder zum vermitteln zusätzlicher Informationen genutzt wird. Aufgrund der Schwierigkeit solcher Texte, die auch für erfahrene Leser ein Problem darstellen können, schließen wir sie vom Zielbereich unseres Systems aus.
Den lokalen Kontext definieren wir hierbei als alle Kanji vor und nach dem aktuell betrachteten innerhalb einer ununterbrochenen Gruppe von Kanji, sowie die unmittelbar auf die aktuelle Kanji-Gruppe folgende Hiragana-Gruppe.
(Info falls nicht bekannt: Japanischkann komplett in Hiragana (allgemeiner Kana) geschrieben werden, jedoch nicht komplett in Kanji, da ein Kanji quasi einen Wortstamm darstellt. Stark vereinfacht treten Kanji in drei "Konfigurationen" auf: einzeln als Nomen, in Gruppen als zusammengesetztes Nomen, mit "Endung" aus Hiragana als adjektiv oder Verb. Zwei unabhängige Nomen können jedoch nicht uhne verbindende Partikel aufeinanderfolgen, somit bestehen japanische Sätze generell aus abwechselnden kleinen Gruppen aus Kanji und Hiragana, wobei sich ein Wort ("wort" im Kontext der Lesungsinferenz) generell nicht über die Grenze seiner Gruppe hinauserstreckt.
Da eine Gruppe jedoch mehrere "Wörter" enthalten kann, planen wir, für die Transskription wie folgt vorzugehen:
Einzelne "Wörter" können wir in in einer Art Wörterbuch, das als offene Datenbank verfügbar ist, nachschlagen, um die entsprechende Lesung zu bekommen. Hierbei gibt es für ein "Wort" im Normalfall eine gängige Lesung, die direkt verwendet werden kann. Ein Kanji kann zwar auf viele Arten gelesen werden, wir nehmen jedoch an, dass die Lesung durch die Konfiguration des Kanji als "Wort" eindeutig festgelegt wird. Während diese Annahme nicht generell gültig ist, ist sie für unsere Zwecke ausreichend.
Da eine Gruppe mehrere "Wörter" enthalten kann, sich ein "Wort" jedoch nicht über seine Gruppe hinaus erstreckt, ist das Problem nun darauf reduziert, eine Gruppe korrekt in "Wörter" aufzuteilen.
Hierfür gehen wir davon aus, dass diejenige Zerlegung die richtige ist, die möglichst lange "Wörter" enthält. (Da das "Alphabet" an Kanji mit einigen tausend Zeichen sehr groß ist, gibt es für jedes einzelne Kanji nur "wenige" Kombinationen, die gültige "Wörter" bilden.
Da es somit recht unwahrscheinlich ist, dass sich eine Gruppe auf mehrere Arten in "Wörter" aus mindestens zwei Kanji zerlegen lässt, und sich in diesem Fall die richtige Zerlegung tatsächlich nur mit globalerem Kontext bestimmen lässt, entscheiden wir uns hier vorerst für einen Greedy Ansatz, von dem wir gute Ergebnisse erwarten:
Von vorne her wird das größte zusammenhängende gültige "Wort" bestimmt (durch sukzessives Nachschlagen, bis das zu lang gewordene "Wort" nicht mehr im "Wörterbuch" enthalten ist) und von der Gruppe entfernt.
Dies wird solange wiederholt, bis der vornestehende Kanji-Teil der Gruppe vollständig verbraucht wurde, wobei ein beliebig großer Teil der trailing Hiragana-Gruppe beim letzten "Wort" mitverwendet werden darf.
Ist dies fehlerfrei möglich, so ist die Zerlegung beendet.
Tritt jedoch der Fall auf, dass so kein gültiges "Wort" mehr gebildet werden kann (auch nicht mit Länge 1), so werden die vorherigen Wörter durch Backtracking solange verkürzt, bis eine gültige Zerlegung gefunden wurde (genaues Vorgehen beim Backtracking noch nicht spezifiziert).
Damit wurde der Text in Gruppen und die Gruppen in "Wörter" zerlegt, und "Wörter" eindeutig auf Lesungen abgebildet. Somit ist das Subproblem gelöst.

4. Augmentations-Subsystem:
Das Eingabebild wird mit korrekt platzierten teiltransparenten Masken für die Lesungen der Kanji belegt. Außerdem werden die Markierungen für "in Training"-Kanji auf die selbe Weise eingefügt.

5. UI-Generierungs-Subsystem:
Hierzu wurden noch keine weiteren Überlegungen angestellt. Da dies jedoch ein standardproblem ist, kann hierfür ein fertiges Framework verwendet werden.

Als primäre Programmiersprache wurde Python gewählt. Falls es der Umfang der Arbeit zulässt wird eine Portierung des Kernsystems mit angepasster UI auf Android angestrebt, um die Anwendung auch mobil nutzen zu können (und durch Nutzung der Kamera live-Augmentierung zu ermöglichen). Außerdem sollte das System möglichst lokal ohne Internetverbindung verwendbar sein.

Falls das System am Ende den Erwartungen entspricht kann es eine echte Hilfe zum Lernen von Kanji sein. Daher wird in Betracht gezogen, das System dann als open source zu veröffentlichen. Dementsprechend soll darauf geachtet werden, dass jegliches externes Material streng nur entsprechend der Lizensbedingungen verwendet wird und dass Fremdleistungen, auch in selbst geschriebenem Code (z.B. Code-Snippets von StackOverflow), entsprechend gekennzeichnet werden.